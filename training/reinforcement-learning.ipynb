{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45e4e620-ab15-4082-b04e-6835ac087b1d",
   "metadata": {},
   "source": [
    "# ReinforcementLearningBot training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6aa3ec3-1483-437f-949f-3f2946ee42f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "import supersuit\n",
    "from torch import nn as nn\n",
    "\n",
    "sys.path.append('..')\n",
    "from env import BlockadeEnv\n",
    "from blockade import Blockade\n",
    "from players.ReinforcementLearningBot import ReinforcementLearningBot\n",
    "from players.OptimizedBot import OptimizedBot\n",
    "from players.HeuristicBot import HeuristicBot\n",
    "from players.RandomBot import RandomBot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59169d4-beac-4fb2-a6ec-6197820e912f",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbe7ce96-bd51-4b3c-8e04-febdfa42f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on:\n",
    "# - example from: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\n",
    "# - wrappers explained in: https://stackoverflow.com/a/73192247\n",
    "\n",
    "# training requires stable-baselines3 2.0.0a8 with modifications\n",
    "# 1) in line 178 of stable_baselines3/common/on_policy_algorithm.py\n",
    "# new_obs, rewards, dones, _, infos = env.step(clipped_actions)\n",
    "# 2) in line 544 of stable_baselines3/common/off_policy_algorithm.py\n",
    "# new_obs, rewards, dones, _, infos = env.step(actions)\n",
    "\n",
    "env = BlockadeEnv(arena_size=15)\n",
    "env = supersuit.pettingzoo_env_to_vec_env_v1(env)\n",
    "env = supersuit.concat_vec_envs_v1(env, 1, base_class='stable_baselines3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6de439-e046-419b-8f97-9ff6e4cd9ea1",
   "metadata": {},
   "source": [
    "# Attempt 1 (default PPO, 20 mln steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dcda907-ba23-4e30-b2ce-f32134bfb1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 743         |\n",
      "|    iterations           | 1000        |\n",
      "|    time_elapsed         | 5509        |\n",
      "|    total_timesteps      | 4096000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007956427 |\n",
      "|    clip_fraction        | 0.039       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0577     |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 92.5        |\n",
      "|    n_updates            | 9990        |\n",
      "|    policy_gradient_loss | -0.00284    |\n",
      "|    value_loss           | 519         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 659         |\n",
      "|    iterations           | 2000        |\n",
      "|    time_elapsed         | 12429       |\n",
      "|    total_timesteps      | 8192000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006830637 |\n",
      "|    clip_fraction        | 0.0149      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0342     |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 411         |\n",
      "|    n_updates            | 19990       |\n",
      "|    policy_gradient_loss | -0.0005     |\n",
      "|    value_loss           | 1.03e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 703          |\n",
      "|    iterations           | 3000         |\n",
      "|    time_elapsed         | 17467        |\n",
      "|    total_timesteps      | 12288000     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047262404 |\n",
      "|    clip_fraction        | 0.0347       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0557      |\n",
      "|    explained_variance   | 0.973        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 142          |\n",
      "|    n_updates            | 29990        |\n",
      "|    policy_gradient_loss | -0.00187     |\n",
      "|    value_loss           | 181          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 731         |\n",
      "|    iterations           | 4000        |\n",
      "|    time_elapsed         | 22405       |\n",
      "|    total_timesteps      | 16384000    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021745047 |\n",
      "|    clip_fraction        | 0.0433      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0566     |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 116         |\n",
      "|    n_updates            | 39990       |\n",
      "|    policy_gradient_loss | -0.00314    |\n",
      "|    value_loss           | 204         |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=20000000, log_interval=1000)\n",
    "model.save('../players/PPO15v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f514e4b4-e499-4e01-a772-6eece7f57191",
   "metadata": {},
   "source": [
    "# Attempt 2 (default A2C, 20 mln steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caa94461-5ddb-4514-a332-be2cba702ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1179     |\n",
      "|    iterations         | 100000   |\n",
      "|    time_elapsed       | 847      |\n",
      "|    total_timesteps    | 1000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0527  |\n",
      "|    explained_variance | 0.915    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99999    |\n",
      "|    policy_loss        | -0.141   |\n",
      "|    value_loss         | 824      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1153      |\n",
      "|    iterations         | 200000    |\n",
      "|    time_elapsed       | 1734      |\n",
      "|    total_timesteps    | 2000000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00112  |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199999    |\n",
      "|    policy_loss        | -9.34e-06 |\n",
      "|    value_loss         | 0.112     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1147      |\n",
      "|    iterations         | 300000    |\n",
      "|    time_elapsed       | 2614      |\n",
      "|    total_timesteps    | 3000000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000505 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299999    |\n",
      "|    policy_loss        | -6.21e-06 |\n",
      "|    value_loss         | 0.00212   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1142      |\n",
      "|    iterations         | 400000    |\n",
      "|    time_elapsed       | 3502      |\n",
      "|    total_timesteps    | 4000000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.61e-05 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399999    |\n",
      "|    policy_loss        | 8.57e-08  |\n",
      "|    value_loss         | 0.196     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1135      |\n",
      "|    iterations         | 500000    |\n",
      "|    time_elapsed       | 4401      |\n",
      "|    total_timesteps    | 5000000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -8.18e-05 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499999    |\n",
      "|    policy_loss        | 1.11e-08  |\n",
      "|    value_loss         | 0.00123   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1128      |\n",
      "|    iterations         | 600000    |\n",
      "|    time_elapsed       | 5317      |\n",
      "|    total_timesteps    | 6000000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.26e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599999    |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 0.000535  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1122      |\n",
      "|    iterations         | 700000    |\n",
      "|    time_elapsed       | 6236      |\n",
      "|    total_timesteps    | 7000000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.23e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699999    |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 0.000381  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1118      |\n",
      "|    iterations         | 800000    |\n",
      "|    time_elapsed       | 7153      |\n",
      "|    total_timesteps    | 8000000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -9.41e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799999    |\n",
      "|    policy_loss        | -2.13e-09 |\n",
      "|    value_loss         | 0.000198  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1115      |\n",
      "|    iterations         | 900000    |\n",
      "|    time_elapsed       | 8069      |\n",
      "|    total_timesteps    | 9000000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.93e-05 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899999    |\n",
      "|    policy_loss        | 6.29e-09  |\n",
      "|    value_loss         | 0.000303  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1111      |\n",
      "|    iterations         | 1000000   |\n",
      "|    time_elapsed       | 8996      |\n",
      "|    total_timesteps    | 10000000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.24e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999999    |\n",
      "|    policy_loss        | 2.6e-09   |\n",
      "|    value_loss         | 0.000141  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1109      |\n",
      "|    iterations         | 1100000   |\n",
      "|    time_elapsed       | 9913      |\n",
      "|    total_timesteps    | 11000000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.19e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099999   |\n",
      "|    policy_loss        | -3.27e-09 |\n",
      "|    value_loss         | 8.84e-05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1107      |\n",
      "|    iterations         | 1200000   |\n",
      "|    time_elapsed       | 10834     |\n",
      "|    total_timesteps    | 12000000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.15e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199999   |\n",
      "|    policy_loss        | 3.2e-09   |\n",
      "|    value_loss         | 8.73e-05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1106      |\n",
      "|    iterations         | 1300000   |\n",
      "|    time_elapsed       | 11751     |\n",
      "|    total_timesteps    | 13000000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.11e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299999   |\n",
      "|    policy_loss        | -1.13e-09 |\n",
      "|    value_loss         | 0.000276  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1105      |\n",
      "|    iterations         | 1400000   |\n",
      "|    time_elapsed       | 12669     |\n",
      "|    total_timesteps    | 14000000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.08e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399999   |\n",
      "|    policy_loss        | -2.66e-09 |\n",
      "|    value_loss         | 0.000223  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1103      |\n",
      "|    iterations         | 1500000   |\n",
      "|    time_elapsed       | 13591     |\n",
      "|    total_timesteps    | 15000000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.05e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499999   |\n",
      "|    policy_loss        | 2.41e-09  |\n",
      "|    value_loss         | 0.000158  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1102      |\n",
      "|    iterations         | 1600000   |\n",
      "|    time_elapsed       | 14510     |\n",
      "|    total_timesteps    | 16000000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.02e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599999   |\n",
      "|    policy_loss        | 2.46e-09  |\n",
      "|    value_loss         | 0.000256  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1101      |\n",
      "|    iterations         | 1700000   |\n",
      "|    time_elapsed       | 15429     |\n",
      "|    total_timesteps    | 17000000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4e-06    |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699999   |\n",
      "|    policy_loss        | -2.42e-09 |\n",
      "|    value_loss         | 0.000159  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1101      |\n",
      "|    iterations         | 1800000   |\n",
      "|    time_elapsed       | 16347     |\n",
      "|    total_timesteps    | 18000000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.98e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799999   |\n",
      "|    policy_loss        | -3.18e-09 |\n",
      "|    value_loss         | 0.000131  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1097      |\n",
      "|    iterations         | 1900000   |\n",
      "|    time_elapsed       | 17316     |\n",
      "|    total_timesteps    | 19000000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.96e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899999   |\n",
      "|    policy_loss        | -3.89e-10 |\n",
      "|    value_loss         | 0.000348  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1093      |\n",
      "|    iterations         | 2000000   |\n",
      "|    time_elapsed       | 18297     |\n",
      "|    total_timesteps    | 20000000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.75e-07 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999999   |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 8.73e-05  |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = A2C('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=20000000, log_interval=100000)\n",
    "model.save('../players/A2C15v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad19eb77-f9ed-49c8-a8d8-32660b1bf15c",
   "metadata": {},
   "source": [
    "# Attempt 3 (default DQN, 20 mln steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afa492bb-6970-4e3e-94ac-d5b236fa8761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.846    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100000   |\n",
      "|    fps              | 2036     |\n",
      "|    time_elapsed     | 159      |\n",
      "|    total_timesteps  | 324226   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 26.3     |\n",
      "|    n_updates        | 34278    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.676    |\n",
      "| time/               |          |\n",
      "|    episodes         | 200000   |\n",
      "|    fps              | 1937     |\n",
      "|    time_elapsed     | 351      |\n",
      "|    total_timesteps  | 681698   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 23.4     |\n",
      "|    n_updates        | 78962    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.485    |\n",
      "| time/               |          |\n",
      "|    episodes         | 300000   |\n",
      "|    fps              | 1887     |\n",
      "|    time_elapsed     | 574      |\n",
      "|    total_timesteps  | 1083990  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 24.8     |\n",
      "|    n_updates        | 129248   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.242    |\n",
      "| time/               |          |\n",
      "|    episodes         | 400000   |\n",
      "|    fps              | 1810     |\n",
      "|    time_elapsed     | 881      |\n",
      "|    total_timesteps  | 1596440  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 15.8     |\n",
      "|    n_updates        | 193304   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 500000   |\n",
      "|    fps              | 1740     |\n",
      "|    time_elapsed     | 1451     |\n",
      "|    total_timesteps  | 2526200  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 10       |\n",
      "|    n_updates        | 309524   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 600000   |\n",
      "|    fps              | 1695     |\n",
      "|    time_elapsed     | 2076     |\n",
      "|    total_timesteps  | 3519856  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.21     |\n",
      "|    n_updates        | 433731   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 700000   |\n",
      "|    fps              | 1671     |\n",
      "|    time_elapsed     | 2609     |\n",
      "|    total_timesteps  | 4360454  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 12.4     |\n",
      "|    n_updates        | 538806   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 800000   |\n",
      "|    fps              | 1644     |\n",
      "|    time_elapsed     | 3261     |\n",
      "|    total_timesteps  | 5362216  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.97     |\n",
      "|    n_updates        | 664026   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 900000   |\n",
      "|    fps              | 1630     |\n",
      "|    time_elapsed     | 3871     |\n",
      "|    total_timesteps  | 6312562  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 15.6     |\n",
      "|    n_updates        | 782820   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1000000  |\n",
      "|    fps              | 1617     |\n",
      "|    time_elapsed     | 4511     |\n",
      "|    total_timesteps  | 7295800  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.71     |\n",
      "|    n_updates        | 905724   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1100000  |\n",
      "|    fps              | 1613     |\n",
      "|    time_elapsed     | 5075     |\n",
      "|    total_timesteps  | 8188492  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.15     |\n",
      "|    n_updates        | 1017311  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1200000  |\n",
      "|    fps              | 1612     |\n",
      "|    time_elapsed     | 5642     |\n",
      "|    total_timesteps  | 9098914  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6        |\n",
      "|    n_updates        | 1131114  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1300000  |\n",
      "|    fps              | 1613     |\n",
      "|    time_elapsed     | 6246     |\n",
      "|    total_timesteps  | 10078206 |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 10.5     |\n",
      "|    n_updates        | 1253525  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1400000  |\n",
      "|    fps              | 1610     |\n",
      "|    time_elapsed     | 6878     |\n",
      "|    total_timesteps  | 11077898 |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.89     |\n",
      "|    n_updates        | 1378487  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1500000  |\n",
      "|    fps              | 1610     |\n",
      "|    time_elapsed     | 7451     |\n",
      "|    total_timesteps  | 12003424 |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 15.7     |\n",
      "|    n_updates        | 1494177  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1600000  |\n",
      "|    fps              | 1607     |\n",
      "|    time_elapsed     | 8118     |\n",
      "|    total_timesteps  | 13047658 |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.71     |\n",
      "|    n_updates        | 1624707  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1700000  |\n",
      "|    fps              | 1605     |\n",
      "|    time_elapsed     | 8796     |\n",
      "|    total_timesteps  | 14121944 |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.13     |\n",
      "|    n_updates        | 1758992  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1800000  |\n",
      "|    fps              | 1605     |\n",
      "|    time_elapsed     | 9537     |\n",
      "|    total_timesteps  | 15308094 |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.15     |\n",
      "|    n_updates        | 1907261  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1900000  |\n",
      "|    fps              | 1604     |\n",
      "|    time_elapsed     | 10255    |\n",
      "|    total_timesteps  | 16453734 |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.79     |\n",
      "|    n_updates        | 2050466  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2000000  |\n",
      "|    fps              | 1603     |\n",
      "|    time_elapsed     | 10953    |\n",
      "|    total_timesteps  | 17563648 |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.85     |\n",
      "|    n_updates        | 2189205  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2100000  |\n",
      "|    fps              | 1601     |\n",
      "|    time_elapsed     | 11743    |\n",
      "|    total_timesteps  | 18810672 |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.55     |\n",
      "|    n_updates        | 2345083  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2200000  |\n",
      "|    fps              | 1600     |\n",
      "|    time_elapsed     | 12463    |\n",
      "|    total_timesteps  | 19950142 |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.53     |\n",
      "|    n_updates        | 2487517  |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = DQN('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=20000000, log_interval=100000)\n",
    "model.save('../players/DQN15v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d25f4d8-2382-47db-8eee-15afe3e2d09c",
   "metadata": {},
   "source": [
    "# Compare bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b038792e-2d65-4ac2-81b3-629136283f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bot_comparison(bot1, bot2, arena_size=15, num_seeds=500, repetitions=2):\n",
    "    opt_win_counter = 0\n",
    "    draw_counter = 0\n",
    "    for seed in trange(num_seeds):\n",
    "        for rep in range(repetitions):\n",
    "            random.seed(seed)\n",
    "            p1 = bot1\n",
    "            p2 = bot2\n",
    "            if rep % 2:\n",
    "                p1, p2 = p2, p1\n",
    "            game = Blockade(player1=p1,\n",
    "                            player2=p2,\n",
    "                            arena_size=arena_size,\n",
    "                            verbose=False)\n",
    "            outcome = game.run_windowless()\n",
    "            if (p1 == bot1 and outcome == 1) or (p2 == bot1 and outcome == 2):\n",
    "                opt_win_counter += 1\n",
    "            elif outcome == 0:\n",
    "                draw_counter += 1\n",
    "\n",
    "    print(f'{bot1} against {bot2} results (arena_size={arena_size}):')\n",
    "    total_games = num_seeds * repetitions\n",
    "    lost_games = total_games - opt_win_counter - draw_counter\n",
    "    print(f'{opt_win_counter}/{total_games} games won ({np.round(opt_win_counter / total_games * 100.0, 2)}%)')\n",
    "    print(f'{draw_counter}/{total_games} draws ({np.round(draw_counter / total_games * 100.0, 2)}%)')\n",
    "    print(f'{lost_games}/{total_games} games lost ({np.round(lost_games / total_games * 100.0, 2)}%)')\n",
    "    \n",
    "    return opt_win_counter, draw_counter, lost_games"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e373e07-853e-4afe-8391-70bd19ae88e4",
   "metadata": {},
   "source": [
    "## vs RandomBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0614cec-02a7-4ea9-863f-847a8616544a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f50a4583bd4623a22c222156daee1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReinforcementLearningBot against RandomBot results (arena_size=15):\n",
      "548/1000 games won (54.8%)\n",
      "79/1000 draws (7.9%)\n",
      "373/1000 games lost (37.3%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(548, 79, 373)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name='../players/PPO15v1', model_type='ppo'), RandomBot(verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f46d201e-c6af-4fd5-bd12-c0b2199f5605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c7662d2e7c4c99b2071a20328a0f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReinforcementLearningBot against RandomBot results (arena_size=15):\n",
      "554/1000 games won (55.4%)\n",
      "86/1000 draws (8.6%)\n",
      "360/1000 games lost (36.0%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(554, 86, 360)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name='../players/A2C15v1', model_type='a2c'), RandomBot(verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec5bf743-26d5-4ee1-9ab0-0ddc0c76b935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fa8246d10c44eaabdb6d623efb3841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReinforcementLearningBot against RandomBot results (arena_size=15):\n",
      "562/1000 games won (56.2%)\n",
      "76/1000 draws (7.6%)\n",
      "362/1000 games lost (36.2%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(562, 76, 362)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name='../players/DQN15v1', model_type='dqn'), RandomBot(verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f2e858-b8dd-41f0-846e-ef3c03749c10",
   "metadata": {},
   "source": [
    "## vs HeuristicBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d796411-98be-4087-a143-619ea1027636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e17266d41a04095a9c9feb28b289412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReinforcementLearningBot against HeuristicBot results (arena_size=15):\n",
      "43/1000 games won (4.3%)\n",
      "109/1000 draws (10.9%)\n",
      "848/1000 games lost (84.8%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(43, 109, 848)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name='../players/PPO15v1', model_type='ppo'), HeuristicBot(verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e4e9ba7-fd9e-4201-9161-528af0bab427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9511c6ad70eb4bb598a496237f14117e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReinforcementLearningBot against HeuristicBot results (arena_size=15):\n",
      "84/1000 games won (8.4%)\n",
      "165/1000 draws (16.5%)\n",
      "751/1000 games lost (75.1%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(84, 165, 751)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name='../players/A2C15v1', model_type='a2c'), HeuristicBot(verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "729d09cb-2054-4129-99ef-67c8bef519aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9dec5d8e5c4197bae4965055d9d2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReinforcementLearningBot against HeuristicBot results (arena_size=15):\n",
      "77/1000 games won (7.7%)\n",
      "123/1000 draws (12.3%)\n",
      "800/1000 games lost (80.0%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(77, 123, 800)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name='../players/DQN15v1', model_type='dqn'), HeuristicBot(verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ee93e6-0ecc-4db0-b1f9-be726f37fa9b",
   "metadata": {},
   "source": [
    "## vs other RL bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1be38f8-70b7-439c-8c91-ee2e69fc7516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28fa0f0db62442d1adfa4898e054bf69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReinforcementLearningBot against ReinforcementLearningBot results (arena_size=15):\n",
      "531/1000 games won (53.1%)\n",
      "69/1000 draws (6.9%)\n",
      "400/1000 games lost (40.0%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(531, 69, 400)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name='../players/PPO15v1', model_type='ppo'), \n",
    "                    ReinforcementLearningBot(verbose=False, model_name='../players/A2C15v1', model_type='a2c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd55a0a7-23ed-4d00-8316-7d08d50c3cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77519747b144eefa763c4baefe93ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReinforcementLearningBot against ReinforcementLearningBot results (arena_size=15):\n",
      "376/1000 games won (37.6%)\n",
      "125/1000 draws (12.5%)\n",
      "499/1000 games lost (49.9%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(376, 125, 499)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name='../players/PPO15v1', model_type='ppo'), \n",
    "                    ReinforcementLearningBot(verbose=False, model_name='../players/DQN15v1', model_type='dqn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bde0b04e-21a0-41fd-84ad-371e92217746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07b04dc416a46379d59796f598ea583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReinforcementLearningBot against ReinforcementLearningBot results (arena_size=15):\n",
      "384/1000 games won (38.4%)\n",
      "103/1000 draws (10.3%)\n",
      "513/1000 games lost (51.3%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(384, 103, 513)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name='../players/A2C15v1', model_type='a2c'), \n",
    "                    ReinforcementLearningBot(verbose=False, model_name='../players/DQN15v1', model_type='dqn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd006aa-bb1a-4149-8b07-470ea6f88198",
   "metadata": {},
   "source": [
    "The best results were achieved by attempt 2 (default A2C algorithm with 20 mln steps). Now the results should be further improved by training A2C with different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29003b4f-a1be-42e2-959e-1055b0f7e859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
