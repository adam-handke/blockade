{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45e4e620-ab15-4082-b04e-6835ac087b1d",
   "metadata": {},
   "source": [
    "# ReinforcementLearningBot training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6aa3ec3-1483-437f-949f-3f2946ee42f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "import supersuit\n",
    "from torch import nn as nn\n",
    "\n",
    "sys.path.append('..')\n",
    "from env import BlockadeEnv\n",
    "from blockade import Blockade\n",
    "from players.ReinforcementLearningBot import ReinforcementLearningBot\n",
    "from players.OptimizedBot import OptimizedBot\n",
    "from players.HeuristicBot import HeuristicBot\n",
    "from players.RandomBot import RandomBot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59169d4-beac-4fb2-a6ec-6197820e912f",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbe7ce96-bd51-4b3c-8e04-febdfa42f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on:\n",
    "# - example from: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\n",
    "# - wrappers explained in: https://stackoverflow.com/a/73192247\n",
    "\n",
    "# training requires stable-baselines3 2.0.0a8 with modifications\n",
    "# 1) in line 178 of stable_baselines3/common/on_policy_algorithm.py\n",
    "# new_obs, rewards, dones, _, infos = env.step(clipped_actions)\n",
    "# 2) in line 544 of stable_baselines3/common/off_policy_algorithm.py\n",
    "# new_obs, rewards, dones, _, infos = env.step(actions)\n",
    "\n",
    "env = BlockadeEnv(arena_size=15)\n",
    "env = supersuit.pettingzoo_env_to_vec_env_v1(env)\n",
    "env = supersuit.concat_vec_envs_v1(env, 1, base_class='stable_baselines3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6de439-e046-419b-8f97-9ff6e4cd9ea1",
   "metadata": {},
   "source": [
    "# Attempt 1 (default PPO, 20 mln steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dcda907-ba23-4e30-b2ce-f32134bfb1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 743         |\n",
      "|    iterations           | 1000        |\n",
      "|    time_elapsed         | 5509        |\n",
      "|    total_timesteps      | 4096000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007956427 |\n",
      "|    clip_fraction        | 0.039       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0577     |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 92.5        |\n",
      "|    n_updates            | 9990        |\n",
      "|    policy_gradient_loss | -0.00284    |\n",
      "|    value_loss           | 519         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 659         |\n",
      "|    iterations           | 2000        |\n",
      "|    time_elapsed         | 12429       |\n",
      "|    total_timesteps      | 8192000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006830637 |\n",
      "|    clip_fraction        | 0.0149      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0342     |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 411         |\n",
      "|    n_updates            | 19990       |\n",
      "|    policy_gradient_loss | -0.0005     |\n",
      "|    value_loss           | 1.03e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 703          |\n",
      "|    iterations           | 3000         |\n",
      "|    time_elapsed         | 17467        |\n",
      "|    total_timesteps      | 12288000     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047262404 |\n",
      "|    clip_fraction        | 0.0347       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0557      |\n",
      "|    explained_variance   | 0.973        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 142          |\n",
      "|    n_updates            | 29990        |\n",
      "|    policy_gradient_loss | -0.00187     |\n",
      "|    value_loss           | 181          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 731         |\n",
      "|    iterations           | 4000        |\n",
      "|    time_elapsed         | 22405       |\n",
      "|    total_timesteps      | 16384000    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021745047 |\n",
      "|    clip_fraction        | 0.0433      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0566     |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 116         |\n",
      "|    n_updates            | 39990       |\n",
      "|    policy_gradient_loss | -0.00314    |\n",
      "|    value_loss           | 204         |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=20000000, log_interval=1000)\n",
    "model.save('../players/PPO15v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f514e4b4-e499-4e01-a772-6eece7f57191",
   "metadata": {},
   "source": [
    "# Attempt 2 (default A2C, 20 mln steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caa94461-5ddb-4514-a332-be2cba702ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1179     |\n",
      "|    iterations         | 100000   |\n",
      "|    time_elapsed       | 847      |\n",
      "|    total_timesteps    | 1000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0527  |\n",
      "|    explained_variance | 0.915    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99999    |\n",
      "|    policy_loss        | -0.141   |\n",
      "|    value_loss         | 824      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1153      |\n",
      "|    iterations         | 200000    |\n",
      "|    time_elapsed       | 1734      |\n",
      "|    total_timesteps    | 2000000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00112  |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199999    |\n",
      "|    policy_loss        | -9.34e-06 |\n",
      "|    value_loss         | 0.112     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1147      |\n",
      "|    iterations         | 300000    |\n",
      "|    time_elapsed       | 2614      |\n",
      "|    total_timesteps    | 3000000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000505 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299999    |\n",
      "|    policy_loss        | -6.21e-06 |\n",
      "|    value_loss         | 0.00212   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1142      |\n",
      "|    iterations         | 400000    |\n",
      "|    time_elapsed       | 3502      |\n",
      "|    total_timesteps    | 4000000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.61e-05 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399999    |\n",
      "|    policy_loss        | 8.57e-08  |\n",
      "|    value_loss         | 0.196     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1135      |\n",
      "|    iterations         | 500000    |\n",
      "|    time_elapsed       | 4401      |\n",
      "|    total_timesteps    | 5000000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -8.18e-05 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499999    |\n",
      "|    policy_loss        | 1.11e-08  |\n",
      "|    value_loss         | 0.00123   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1128      |\n",
      "|    iterations         | 600000    |\n",
      "|    time_elapsed       | 5317      |\n",
      "|    total_timesteps    | 6000000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.26e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599999    |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 0.000535  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1122      |\n",
      "|    iterations         | 700000    |\n",
      "|    time_elapsed       | 6236      |\n",
      "|    total_timesteps    | 7000000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.23e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699999    |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 0.000381  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1118      |\n",
      "|    iterations         | 800000    |\n",
      "|    time_elapsed       | 7153      |\n",
      "|    total_timesteps    | 8000000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -9.41e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799999    |\n",
      "|    policy_loss        | -2.13e-09 |\n",
      "|    value_loss         | 0.000198  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1115      |\n",
      "|    iterations         | 900000    |\n",
      "|    time_elapsed       | 8069      |\n",
      "|    total_timesteps    | 9000000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.93e-05 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899999    |\n",
      "|    policy_loss        | 6.29e-09  |\n",
      "|    value_loss         | 0.000303  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1111      |\n",
      "|    iterations         | 1000000   |\n",
      "|    time_elapsed       | 8996      |\n",
      "|    total_timesteps    | 10000000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.24e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999999    |\n",
      "|    policy_loss        | 2.6e-09   |\n",
      "|    value_loss         | 0.000141  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1109      |\n",
      "|    iterations         | 1100000   |\n",
      "|    time_elapsed       | 9913      |\n",
      "|    total_timesteps    | 11000000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.19e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099999   |\n",
      "|    policy_loss        | -3.27e-09 |\n",
      "|    value_loss         | 8.84e-05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1107      |\n",
      "|    iterations         | 1200000   |\n",
      "|    time_elapsed       | 10834     |\n",
      "|    total_timesteps    | 12000000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.15e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199999   |\n",
      "|    policy_loss        | 3.2e-09   |\n",
      "|    value_loss         | 8.73e-05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1106      |\n",
      "|    iterations         | 1300000   |\n",
      "|    time_elapsed       | 11751     |\n",
      "|    total_timesteps    | 13000000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.11e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299999   |\n",
      "|    policy_loss        | -1.13e-09 |\n",
      "|    value_loss         | 0.000276  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1105      |\n",
      "|    iterations         | 1400000   |\n",
      "|    time_elapsed       | 12669     |\n",
      "|    total_timesteps    | 14000000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.08e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399999   |\n",
      "|    policy_loss        | -2.66e-09 |\n",
      "|    value_loss         | 0.000223  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1103      |\n",
      "|    iterations         | 1500000   |\n",
      "|    time_elapsed       | 13591     |\n",
      "|    total_timesteps    | 15000000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.05e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499999   |\n",
      "|    policy_loss        | 2.41e-09  |\n",
      "|    value_loss         | 0.000158  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1102      |\n",
      "|    iterations         | 1600000   |\n",
      "|    time_elapsed       | 14510     |\n",
      "|    total_timesteps    | 16000000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.02e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599999   |\n",
      "|    policy_loss        | 2.46e-09  |\n",
      "|    value_loss         | 0.000256  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1101      |\n",
      "|    iterations         | 1700000   |\n",
      "|    time_elapsed       | 15429     |\n",
      "|    total_timesteps    | 17000000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4e-06    |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699999   |\n",
      "|    policy_loss        | -2.42e-09 |\n",
      "|    value_loss         | 0.000159  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1101      |\n",
      "|    iterations         | 1800000   |\n",
      "|    time_elapsed       | 16347     |\n",
      "|    total_timesteps    | 18000000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.98e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799999   |\n",
      "|    policy_loss        | -3.18e-09 |\n",
      "|    value_loss         | 0.000131  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1097      |\n",
      "|    iterations         | 1900000   |\n",
      "|    time_elapsed       | 17316     |\n",
      "|    total_timesteps    | 19000000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.96e-06 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899999   |\n",
      "|    policy_loss        | -3.89e-10 |\n",
      "|    value_loss         | 0.000348  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1093      |\n",
      "|    iterations         | 2000000   |\n",
      "|    time_elapsed       | 18297     |\n",
      "|    total_timesteps    | 20000000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.75e-07 |\n",
      "|    explained_variance | 1         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999999   |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 8.73e-05  |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = A2C('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=20000000, log_interval=100000)\n",
    "model.save('../players/A2C15v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad19eb77-f9ed-49c8-a8d8-32660b1bf15c",
   "metadata": {},
   "source": [
    "# Attempt 3 (default DQN, 20 mln steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afa492bb-6970-4e3e-94ac-d5b236fa8761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.846    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100000   |\n",
      "|    fps              | 2036     |\n",
      "|    time_elapsed     | 159      |\n",
      "|    total_timesteps  | 324226   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 26.3     |\n",
      "|    n_updates        | 34278    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.676    |\n",
      "| time/               |          |\n",
      "|    episodes         | 200000   |\n",
      "|    fps              | 1937     |\n",
      "|    time_elapsed     | 351      |\n",
      "|    total_timesteps  | 681698   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 23.4     |\n",
      "|    n_updates        | 78962    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.485    |\n",
      "| time/               |          |\n",
      "|    episodes         | 300000   |\n",
      "|    fps              | 1887     |\n",
      "|    time_elapsed     | 574      |\n",
      "|    total_timesteps  | 1083990  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 24.8     |\n",
      "|    n_updates        | 129248   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.242    |\n",
      "| time/               |          |\n",
      "|    episodes         | 400000   |\n",
      "|    fps              | 1810     |\n",
      "|    time_elapsed     | 881      |\n",
      "|    total_timesteps  | 1596440  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 15.8     |\n",
      "|    n_updates        | 193304   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 500000   |\n",
      "|    fps              | 1740     |\n",
      "|    time_elapsed     | 1451     |\n",
      "|    total_timesteps  | 2526200  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 10       |\n",
      "|    n_updates        | 309524   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 600000   |\n",
      "|    fps              | 1695     |\n",
      "|    time_elapsed     | 2076     |\n",
      "|    total_timesteps  | 3519856  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.21     |\n",
      "|    n_updates        | 433731   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 700000   |\n",
      "|    fps              | 1671     |\n",
      "|    time_elapsed     | 2609     |\n",
      "|    total_timesteps  | 4360454  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 12.4     |\n",
      "|    n_updates        | 538806   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 800000   |\n",
      "|    fps              | 1644     |\n",
      "|    time_elapsed     | 3261     |\n",
      "|    total_timesteps  | 5362216  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.97     |\n",
      "|    n_updates        | 664026   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 900000   |\n",
      "|    fps              | 1630     |\n",
      "|    time_elapsed     | 3871     |\n",
      "|    total_timesteps  | 6312562  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 15.6     |\n",
      "|    n_updates        | 782820   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1000000  |\n",
      "|    fps              | 1617     |\n",
      "|    time_elapsed     | 4511     |\n",
      "|    total_timesteps  | 7295800  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.71     |\n",
      "|    n_updates        | 905724   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1100000  |\n",
      "|    fps              | 1613     |\n",
      "|    time_elapsed     | 5075     |\n",
      "|    total_timesteps  | 8188492  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.15     |\n",
      "|    n_updates        | 1017311  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1200000  |\n",
      "|    fps              | 1612     |\n",
      "|    time_elapsed     | 5642     |\n",
      "|    total_timesteps  | 9098914  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6        |\n",
      "|    n_updates        | 1131114  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1300000  |\n",
      "|    fps              | 1613     |\n",
      "|    time_elapsed     | 6246     |\n",
      "|    total_timesteps  | 10078206 |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 10.5     |\n",
      "|    n_updates        | 1253525  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1400000  |\n",
      "|    fps              | 1610     |\n",
      "|    time_elapsed     | 6878     |\n",
      "|    total_timesteps  | 11077898 |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.89     |\n",
      "|    n_updates        | 1378487  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1500000  |\n",
      "|    fps              | 1610     |\n",
      "|    time_elapsed     | 7451     |\n",
      "|    total_timesteps  | 12003424 |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 15.7     |\n",
      "|    n_updates        | 1494177  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1600000  |\n",
      "|    fps              | 1607     |\n",
      "|    time_elapsed     | 8118     |\n",
      "|    total_timesteps  | 13047658 |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.71     |\n",
      "|    n_updates        | 1624707  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1700000  |\n",
      "|    fps              | 1605     |\n",
      "|    time_elapsed     | 8796     |\n",
      "|    total_timesteps  | 14121944 |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.13     |\n",
      "|    n_updates        | 1758992  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1800000  |\n",
      "|    fps              | 1605     |\n",
      "|    time_elapsed     | 9537     |\n",
      "|    total_timesteps  | 15308094 |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.15     |\n",
      "|    n_updates        | 1907261  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1900000  |\n",
      "|    fps              | 1604     |\n",
      "|    time_elapsed     | 10255    |\n",
      "|    total_timesteps  | 16453734 |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.79     |\n",
      "|    n_updates        | 2050466  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2000000  |\n",
      "|    fps              | 1603     |\n",
      "|    time_elapsed     | 10953    |\n",
      "|    total_timesteps  | 17563648 |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.85     |\n",
      "|    n_updates        | 2189205  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2100000  |\n",
      "|    fps              | 1601     |\n",
      "|    time_elapsed     | 11743    |\n",
      "|    total_timesteps  | 18810672 |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.55     |\n",
      "|    n_updates        | 2345083  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2200000  |\n",
      "|    fps              | 1600     |\n",
      "|    time_elapsed     | 12463    |\n",
      "|    total_timesteps  | 19950142 |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.53     |\n",
      "|    n_updates        | 2487517  |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = DQN('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=20000000, log_interval=100000)\n",
    "model.save('../players/DQN15v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d25f4d8-2382-47db-8eee-15afe3e2d09c",
   "metadata": {},
   "source": [
    "# Compare bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b038792e-2d65-4ac2-81b3-629136283f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bot_comparison(bot1, bot2, arena_size=15, num_seeds=500, repetitions=2, verbose=True):\n",
    "    opt_win_counter = 0\n",
    "    draw_counter = 0\n",
    "    \n",
    "    if verbose:\n",
    "        for seed in trange(num_seeds):\n",
    "            for rep in range(repetitions):\n",
    "                random.seed(seed)\n",
    "                p1 = bot1\n",
    "                p2 = bot2\n",
    "                if rep % 2:\n",
    "                    p1, p2 = p2, p1\n",
    "                game = Blockade(player1=p1,\n",
    "                                player2=p2,\n",
    "                                arena_size=arena_size,\n",
    "                                verbose=False)\n",
    "                outcome = game.run_windowless()\n",
    "                if (p1 == bot1 and outcome == 1) or (p2 == bot1 and outcome == 2):\n",
    "                    opt_win_counter += 1\n",
    "                elif outcome == 0:\n",
    "                    draw_counter += 1\n",
    "    else:\n",
    "        for seed in range(num_seeds):\n",
    "            for rep in range(repetitions):\n",
    "                random.seed(seed)\n",
    "                p1 = bot1\n",
    "                p2 = bot2\n",
    "                if rep % 2:\n",
    "                    p1, p2 = p2, p1\n",
    "                game = Blockade(player1=p1,\n",
    "                                player2=p2,\n",
    "                                arena_size=arena_size,\n",
    "                                verbose=False)\n",
    "                outcome = game.run_windowless()\n",
    "                if (p1 == bot1 and outcome == 1) or (p2 == bot1 and outcome == 2):\n",
    "                    opt_win_counter += 1\n",
    "                elif outcome == 0:\n",
    "                    draw_counter += 1\n",
    "        \n",
    "    \n",
    "    \n",
    "    total_games = num_seeds * repetitions\n",
    "    lost_games = total_games - opt_win_counter - draw_counter\n",
    "    if verbose:\n",
    "        print(f'{bot1} against {bot2} results (arena_size={arena_size}):')\n",
    "        print(f'{opt_win_counter}/{total_games} games won ({np.round(opt_win_counter / total_games * 100.0, 2)}%)')\n",
    "        print(f'{draw_counter}/{total_games} draws ({np.round(draw_counter / total_games * 100.0, 2)}%)')\n",
    "        print(f'{lost_games}/{total_games} games lost ({np.round(lost_games / total_games * 100.0, 2)}%)')\n",
    "    \n",
    "    return opt_win_counter, draw_counter, lost_games"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e373e07-853e-4afe-8391-70bd19ae88e4",
   "metadata": {},
   "source": [
    "## vs RandomBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0614cec-02a7-4ea9-863f-847a8616544a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f50a4583bd4623a22c222156daee1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReinforcementLearningBot against RandomBot results (arena_size=15):\n",
      "548/1000 games won (54.8%)\n",
      "79/1000 draws (7.9%)\n",
      "373/1000 games lost (37.3%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(548, 79, 373)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name='../players/PPO15v1', model_type='ppo'), RandomBot(verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f46d201e-c6af-4fd5-bd12-c0b2199f5605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c7662d2e7c4c99b2071a20328a0f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReinforcementLearningBot against RandomBot results (arena_size=15):\n",
      "554/1000 games won (55.4%)\n",
      "86/1000 draws (8.6%)\n",
      "360/1000 games lost (36.0%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(554, 86, 360)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name='../players/A2C15v1', model_type='a2c'), RandomBot(verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec5bf743-26d5-4ee1-9ab0-0ddc0c76b935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fa8246d10c44eaabdb6d623efb3841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReinforcementLearningBot against RandomBot results (arena_size=15):\n",
      "562/1000 games won (56.2%)\n",
      "76/1000 draws (7.6%)\n",
      "362/1000 games lost (36.2%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(562, 76, 362)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name='../players/DQN15v1', model_type='dqn'), RandomBot(verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f2e858-b8dd-41f0-846e-ef3c03749c10",
   "metadata": {},
   "source": [
    "## vs HeuristicBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d796411-98be-4087-a143-619ea1027636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e17266d41a04095a9c9feb28b289412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReinforcementLearningBot against HeuristicBot results (arena_size=15):\n",
      "43/1000 games won (4.3%)\n",
      "109/1000 draws (10.9%)\n",
      "848/1000 games lost (84.8%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(43, 109, 848)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name='../players/PPO15v1', model_type='ppo'), HeuristicBot(verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e4e9ba7-fd9e-4201-9161-528af0bab427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9511c6ad70eb4bb598a496237f14117e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReinforcementLearningBot against HeuristicBot results (arena_size=15):\n",
      "84/1000 games won (8.4%)\n",
      "165/1000 draws (16.5%)\n",
      "751/1000 games lost (75.1%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(84, 165, 751)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name='../players/A2C15v1', model_type='a2c'), HeuristicBot(verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "729d09cb-2054-4129-99ef-67c8bef519aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9dec5d8e5c4197bae4965055d9d2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReinforcementLearningBot against HeuristicBot results (arena_size=15):\n",
      "77/1000 games won (7.7%)\n",
      "123/1000 draws (12.3%)\n",
      "800/1000 games lost (80.0%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(77, 123, 800)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name='../players/DQN15v1', model_type='dqn'), HeuristicBot(verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ee93e6-0ecc-4db0-b1f9-be726f37fa9b",
   "metadata": {},
   "source": [
    "## vs other RL bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1be38f8-70b7-439c-8c91-ee2e69fc7516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28fa0f0db62442d1adfa4898e054bf69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReinforcementLearningBot against ReinforcementLearningBot results (arena_size=15):\n",
      "531/1000 games won (53.1%)\n",
      "69/1000 draws (6.9%)\n",
      "400/1000 games lost (40.0%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(531, 69, 400)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name='../players/PPO15v1', model_type='ppo'), \n",
    "                    ReinforcementLearningBot(verbose=False, model_name='../players/A2C15v1', model_type='a2c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd55a0a7-23ed-4d00-8316-7d08d50c3cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77519747b144eefa763c4baefe93ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReinforcementLearningBot against ReinforcementLearningBot results (arena_size=15):\n",
      "376/1000 games won (37.6%)\n",
      "125/1000 draws (12.5%)\n",
      "499/1000 games lost (49.9%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(376, 125, 499)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name='../players/PPO15v1', model_type='ppo'), \n",
    "                    ReinforcementLearningBot(verbose=False, model_name='../players/DQN15v1', model_type='dqn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bde0b04e-21a0-41fd-84ad-371e92217746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07b04dc416a46379d59796f598ea583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReinforcementLearningBot against ReinforcementLearningBot results (arena_size=15):\n",
      "384/1000 games won (38.4%)\n",
      "103/1000 draws (10.3%)\n",
      "513/1000 games lost (51.3%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(384, 103, 513)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name='../players/A2C15v1', model_type='a2c'), \n",
    "                    ReinforcementLearningBot(verbose=False, model_name='../players/DQN15v1', model_type='dqn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd006aa-bb1a-4149-8b07-470ea6f88198",
   "metadata": {},
   "source": [
    "The best results were achieved by attempt 2 (default A2C algorithm with 20 mln steps). Now the results should be further improved by training A2C with different hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc1dd6-b955-4be6-bf8f-3bcf95093dd9",
   "metadata": {},
   "source": [
    "# Tuning A2C hyperparameters\n",
    "\n",
    "* Only 1 mln training steps for faster evaluation.\n",
    "* Score: 3x wins + 1x draws + 0x loses against RandomBot.\n",
    "* Based on: https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/rl_zoo3/hyperparams_opt.py#L147 and https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html#parameters\n",
    "* Starting with grid search of 3 main parameters: gamma, learning_rate, net_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4f26340-101f-473a-91f9-7b09ecff87c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/64\tA2C gamma=0.9, learning_rate=1e-05, net_arch=[32, 32]: 379 wins, 61 draws, 560 loses; score: 1198\n",
      "2/64\tA2C gamma=0.9, learning_rate=1e-05, net_arch=[64, 64]: 400 wins, 59 draws, 541 loses; score: 1259\n",
      "3/64\tA2C gamma=0.9, learning_rate=1e-05, net_arch=[128, 128]: 359 wins, 112 draws, 529 loses; score: 1189\n",
      "4/64\tA2C gamma=0.9, learning_rate=1e-05, net_arch=[128, 64]: 357 wins, 134 draws, 509 loses; score: 1205\n",
      "5/64\tA2C gamma=0.9, learning_rate=0.0001, net_arch=[32, 32]: 431 wins, 78 draws, 491 loses; score: 1371\n",
      "6/64\tA2C gamma=0.9, learning_rate=0.0001, net_arch=[64, 64]: 434 wins, 75 draws, 491 loses; score: 1377\n",
      "7/64\tA2C gamma=0.9, learning_rate=0.0001, net_arch=[128, 128]: 442 wins, 105 draws, 453 loses; score: 1431\n",
      "8/64\tA2C gamma=0.9, learning_rate=0.0001, net_arch=[128, 64]: 480 wins, 88 draws, 432 loses; score: 1528\n",
      "9/64\tA2C gamma=0.9, learning_rate=0.0007, net_arch=[32, 32]: 508 wins, 116 draws, 376 loses; score: 1640\n",
      "10/64\tA2C gamma=0.9, learning_rate=0.0007, net_arch=[64, 64]: 538 wins, 84 draws, 378 loses; score: 1698\n",
      "11/64\tA2C gamma=0.9, learning_rate=0.0007, net_arch=[128, 128]: 440 wins, 99 draws, 461 loses; score: 1419\n",
      "12/64\tA2C gamma=0.9, learning_rate=0.0007, net_arch=[128, 64]: 527 wins, 71 draws, 402 loses; score: 1652\n",
      "13/64\tA2C gamma=0.9, learning_rate=0.001, net_arch=[32, 32]: 496 wins, 80 draws, 424 loses; score: 1568\n",
      "14/64\tA2C gamma=0.9, learning_rate=0.001, net_arch=[64, 64]: 503 wins, 93 draws, 404 loses; score: 1602\n",
      "15/64\tA2C gamma=0.9, learning_rate=0.001, net_arch=[128, 128]: 369 wins, 86 draws, 545 loses; score: 1193\n",
      "16/64\tA2C gamma=0.9, learning_rate=0.001, net_arch=[128, 64]: 343 wins, 79 draws, 578 loses; score: 1108\n",
      "17/64\tA2C gamma=0.95, learning_rate=1e-05, net_arch=[32, 32]: 354 wins, 155 draws, 491 loses; score: 1217\n",
      "18/64\tA2C gamma=0.95, learning_rate=1e-05, net_arch=[64, 64]: 394 wins, 90 draws, 516 loses; score: 1272\n",
      "19/64\tA2C gamma=0.95, learning_rate=1e-05, net_arch=[128, 128]: 359 wins, 130 draws, 511 loses; score: 1207\n",
      "20/64\tA2C gamma=0.95, learning_rate=1e-05, net_arch=[128, 64]: 356 wins, 123 draws, 521 loses; score: 1191\n",
      "21/64\tA2C gamma=0.95, learning_rate=0.0001, net_arch=[32, 32]: 396 wins, 72 draws, 532 loses; score: 1260\n",
      "22/64\tA2C gamma=0.95, learning_rate=0.0001, net_arch=[64, 64]: 416 wins, 85 draws, 499 loses; score: 1333\n",
      "23/64\tA2C gamma=0.95, learning_rate=0.0001, net_arch=[128, 128]: 470 wins, 114 draws, 416 loses; score: 1524\n",
      "24/64\tA2C gamma=0.95, learning_rate=0.0001, net_arch=[128, 64]: 417 wins, 119 draws, 464 loses; score: 1370\n",
      "25/64\tA2C gamma=0.95, learning_rate=0.0007, net_arch=[32, 32]: 539 wins, 56 draws, 405 loses; score: 1673\n",
      "26/64\tA2C gamma=0.95, learning_rate=0.0007, net_arch=[64, 64]: 505 wins, 87 draws, 408 loses; score: 1602\n",
      "27/64\tA2C gamma=0.95, learning_rate=0.0007, net_arch=[128, 128]: 523 wins, 83 draws, 394 loses; score: 1652\n",
      "28/64\tA2C gamma=0.95, learning_rate=0.0007, net_arch=[128, 64]: 423 wins, 90 draws, 487 loses; score: 1359\n",
      "29/64\tA2C gamma=0.95, learning_rate=0.001, net_arch=[32, 32]: 384 wins, 110 draws, 506 loses; score: 1262\n",
      "30/64\tA2C gamma=0.95, learning_rate=0.001, net_arch=[64, 64]: 559 wins, 83 draws, 358 loses; score: 1760\n",
      "31/64\tA2C gamma=0.95, learning_rate=0.001, net_arch=[128, 128]: 487 wins, 90 draws, 423 loses; score: 1551\n",
      "32/64\tA2C gamma=0.95, learning_rate=0.001, net_arch=[128, 64]: 503 wins, 88 draws, 409 loses; score: 1597\n",
      "33/64\tA2C gamma=0.99, learning_rate=1e-05, net_arch=[32, 32]: 371 wins, 64 draws, 565 loses; score: 1177\n",
      "34/64\tA2C gamma=0.99, learning_rate=1e-05, net_arch=[64, 64]: 366 wins, 72 draws, 562 loses; score: 1170\n",
      "35/64\tA2C gamma=0.99, learning_rate=1e-05, net_arch=[128, 128]: 434 wins, 79 draws, 487 loses; score: 1381\n",
      "36/64\tA2C gamma=0.99, learning_rate=1e-05, net_arch=[128, 64]: 418 wins, 77 draws, 505 loses; score: 1331\n",
      "37/64\tA2C gamma=0.99, learning_rate=0.0001, net_arch=[32, 32]: 423 wins, 70 draws, 507 loses; score: 1339\n",
      "38/64\tA2C gamma=0.99, learning_rate=0.0001, net_arch=[64, 64]: 489 wins, 76 draws, 435 loses; score: 1543\n",
      "39/64\tA2C gamma=0.99, learning_rate=0.0001, net_arch=[128, 128]: 432 wins, 97 draws, 471 loses; score: 1393\n",
      "40/64\tA2C gamma=0.99, learning_rate=0.0001, net_arch=[128, 64]: 370 wins, 80 draws, 550 loses; score: 1190\n",
      "41/64\tA2C gamma=0.99, learning_rate=0.0007, net_arch=[32, 32]: 463 wins, 77 draws, 460 loses; score: 1466\n",
      "42/64\tA2C gamma=0.99, learning_rate=0.0007, net_arch=[64, 64]: 515 wins, 110 draws, 375 loses; score: 1655\n",
      "43/64\tA2C gamma=0.99, learning_rate=0.0007, net_arch=[128, 128]: 488 wins, 75 draws, 437 loses; score: 1539\n",
      "44/64\tA2C gamma=0.99, learning_rate=0.0007, net_arch=[128, 64]: 469 wins, 85 draws, 446 loses; score: 1492\n",
      "45/64\tA2C gamma=0.99, learning_rate=0.001, net_arch=[32, 32]: 482 wins, 114 draws, 404 loses; score: 1560\n",
      "46/64\tA2C gamma=0.99, learning_rate=0.001, net_arch=[64, 64]: 461 wins, 96 draws, 443 loses; score: 1479\n",
      "47/64\tA2C gamma=0.99, learning_rate=0.001, net_arch=[128, 128]: 472 wins, 80 draws, 448 loses; score: 1496\n",
      "48/64\tA2C gamma=0.99, learning_rate=0.001, net_arch=[128, 64]: 472 wins, 79 draws, 449 loses; score: 1495\n",
      "49/64\tA2C gamma=0.999, learning_rate=1e-05, net_arch=[32, 32]: 360 wins, 132 draws, 508 loses; score: 1212\n",
      "50/64\tA2C gamma=0.999, learning_rate=1e-05, net_arch=[64, 64]: 365 wins, 72 draws, 563 loses; score: 1167\n",
      "51/64\tA2C gamma=0.999, learning_rate=1e-05, net_arch=[128, 128]: 409 wins, 72 draws, 519 loses; score: 1299\n",
      "52/64\tA2C gamma=0.999, learning_rate=1e-05, net_arch=[128, 64]: 389 wins, 61 draws, 550 loses; score: 1228\n",
      "53/64\tA2C gamma=0.999, learning_rate=0.0001, net_arch=[32, 32]: 437 wins, 99 draws, 464 loses; score: 1410\n",
      "54/64\tA2C gamma=0.999, learning_rate=0.0001, net_arch=[64, 64]: 435 wins, 105 draws, 460 loses; score: 1410\n",
      "55/64\tA2C gamma=0.999, learning_rate=0.0001, net_arch=[128, 128]: 472 wins, 92 draws, 436 loses; score: 1508\n",
      "56/64\tA2C gamma=0.999, learning_rate=0.0001, net_arch=[128, 64]: 405 wins, 98 draws, 497 loses; score: 1313\n",
      "57/64\tA2C gamma=0.999, learning_rate=0.0007, net_arch=[32, 32]: 480 wins, 93 draws, 427 loses; score: 1533\n",
      "58/64\tA2C gamma=0.999, learning_rate=0.0007, net_arch=[64, 64]: 420 wins, 97 draws, 483 loses; score: 1357\n",
      "59/64\tA2C gamma=0.999, learning_rate=0.0007, net_arch=[128, 128]: 466 wins, 80 draws, 454 loses; score: 1478\n",
      "60/64\tA2C gamma=0.999, learning_rate=0.0007, net_arch=[128, 64]: 462 wins, 82 draws, 456 loses; score: 1468\n",
      "61/64\tA2C gamma=0.999, learning_rate=0.001, net_arch=[32, 32]: 526 wins, 64 draws, 410 loses; score: 1642\n",
      "62/64\tA2C gamma=0.999, learning_rate=0.001, net_arch=[64, 64]: 426 wins, 82 draws, 492 loses; score: 1360\n",
      "63/64\tA2C gamma=0.999, learning_rate=0.001, net_arch=[128, 128]: 402 wins, 93 draws, 505 loses; score: 1299\n",
      "64/64\tA2C gamma=0.999, learning_rate=0.001, net_arch=[128, 64]: 470 wins, 93 draws, 437 loses; score: 1503\n",
      "Best model:  {'gamma': 0.95, 'learning_rate': 0.001, 'net_arch': [64, 64]}\n",
      "Best model score: 1760\n"
     ]
    }
   ],
   "source": [
    "best_model = {'gamma': None, 'learning_rate': None, 'net_arch': None}\n",
    "best_score = 0\n",
    "counter = 1\n",
    "\n",
    "for gamma in [0.9, 0.95, 0.99, 0.999]:\n",
    "    for learning_rate in [0.00001, 0.0001, 0.0007, 0.001]:\n",
    "        for net_arch in [[32, 32], [64, 64], [128, 128], [128, 64]]:\n",
    "            model = A2C('MlpPolicy', env, verbose=0,\n",
    "                        gamma=gamma, learning_rate=learning_rate, policy_kwargs={'net_arch': net_arch})\n",
    "            model.learn(total_timesteps=1000000)\n",
    "            bot_path = f'../players/A2C15_g{gamma}_lr{learning_rate}_net{net_arch}'\n",
    "            model.save(bot_path)\n",
    "            \n",
    "            wins, draws, loses = make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name=bot_path, model_type='a2c'), \n",
    "                                                     RandomBot(verbose=False), verbose=False)\n",
    "            score = 3*wins + draws\n",
    "            \n",
    "            print(f'{counter}/64\\tA2C gamma={gamma}, learning_rate={learning_rate}, net_arch={net_arch}: {wins} wins, {draws} draws, {loses} loses; score: {score}')\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_model = {'gamma': gamma, 'learning_rate': learning_rate, 'net_arch': net_arch}\n",
    "                \n",
    "            counter += 1\n",
    "            \n",
    "print('Best model: ', best_model)\n",
    "print(f'Best model score: {best_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451f43d1-b082-4509-9136-46402761f24f",
   "metadata": {},
   "source": [
    "Tuning of some other hyperparameters without grid search.\n",
    "* n_steps\n",
    "* activation_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b05638b-fc8a-41b1-9f3c-8fbfd74029d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2C n_steps=5: 509 wins, 97 draws, 394 loses; score: 1624\n",
      "A2C n_steps=8: 474 wins, 87 draws, 439 loses; score: 1509\n",
      "A2C n_steps=16: 444 wins, 77 draws, 479 loses; score: 1409\n",
      "A2C n_steps=32: 363 wins, 127 draws, 510 loses; score: 1216\n",
      "A2C n_steps=64: 501 wins, 109 draws, 390 loses; score: 1612\n",
      "A2C n_steps=128: 532 wins, 92 draws, 376 loses; score: 1688\n",
      "Best n_steps:  128\n",
      "Best model score: 1688\n"
     ]
    }
   ],
   "source": [
    "best_n_steps = None\n",
    "best_score = 0\n",
    "\n",
    "for n_steps in [5, 8, 16, 32, 64, 128]:\n",
    "    model = A2C('MlpPolicy', env, verbose=0,\n",
    "                gamma=0.95, learning_rate=0.001, policy_kwargs={'net_arch': [64, 64]})\n",
    "    model.learn(total_timesteps=1000000)\n",
    "    bot_path = f'../players/A2C15_n{n_steps}'\n",
    "    model.save(bot_path)\n",
    "\n",
    "    wins, draws, loses = make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name=bot_path, model_type='a2c'), \n",
    "                                             RandomBot(verbose=False), verbose=False)\n",
    "    score = 3*wins + draws\n",
    "\n",
    "    print(f'A2C n_steps={n_steps}: {wins} wins, {draws} draws, {loses} loses; score: {score}')\n",
    "\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_n_steps = n_steps\n",
    "            \n",
    "print('Best n_steps: ', best_n_steps)\n",
    "print(f'Best model score: {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b62d24ee-d815-4aee-b3ee-2e05ae6f719c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2C activation_fn=Tanh: 478 wins, 97 draws, 425 loses; score: 1531\n",
      "A2C activation_fn=ReLU: 465 wins, 100 draws, 435 loses; score: 1495\n",
      "A2C activation_fn=ELU: 466 wins, 119 draws, 415 loses; score: 1517\n",
      "A2C activation_fn=LeakyReLU: 440 wins, 99 draws, 461 loses; score: 1419\n",
      "Best activation_fn:  <class 'torch.nn.modules.activation.Tanh'>\n",
      "Best model score: 1531\n"
     ]
    }
   ],
   "source": [
    "best_activation_fn = None\n",
    "best_score = 0\n",
    "\n",
    "for activation_fn in [nn.Tanh, nn.ReLU, nn.ELU, nn.LeakyReLU]:\n",
    "    model = A2C('MlpPolicy', env, verbose=0, n_steps=best_n_steps,\n",
    "                gamma=0.95, learning_rate=0.001, policy_kwargs={'net_arch': [64, 64], 'activation_fn': activation_fn})\n",
    "    model.learn(total_timesteps=1000000)\n",
    "    activation_str = str(activation_fn)[(str(activation_fn).index('activation')+11):-2]\n",
    "    bot_path = f'../players/A2C15_fn{activation_str}'\n",
    "    model.save(bot_path)\n",
    "\n",
    "    wins, draws, loses = make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name=bot_path, model_type='a2c'), \n",
    "                                             RandomBot(verbose=False), verbose=False)\n",
    "    score = 3*wins + draws\n",
    "\n",
    "    print(f'A2C activation_fn={activation_str}: {wins} wins, {draws} draws, {loses} loses; score: {score}')\n",
    "\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_activation_fn = activation_fn\n",
    "            \n",
    "print('Best activation_fn: ', best_activation_fn)\n",
    "print(f'Best model score: {best_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334516e7-bd9b-4eaf-92aa-58b7add23856",
   "metadata": {},
   "source": [
    "# Attempt 4 (tuned A2C, 20 mln steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2475e203-40a6-492e-867d-105438d40cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C('MlpPolicy', env, verbose=1, n_steps=128, gamma=0.95, learning_rate=0.001, \n",
    "            policy_kwargs={'net_arch': [64, 64], 'activation_fn': nn.Tanh})\n",
    "model.learn(total_timesteps=20000000, log_interval=100000)\n",
    "model.save('../players/A2C15v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad911f7-8a24-49e0-a089-2d254aaf3762",
   "metadata": {},
   "source": [
    "## Compare bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce6e8c-ef68-4279-996d-ae7e703e7208",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name='../players/A2C15v2', model_type='a2c'), RandomBot(verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddaa211-9930-4988-ba70-8d3e0f4ece1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name='../players/A2C15v2', model_type='a2c'), \n",
    "                    ReinforcementLearningBot(verbose=False, model_name='../players/A2C15v1', model_type='a2c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defe7148-abfd-42d1-b349-63715090d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name='../players/A2C15v2', model_type='a2c'), \n",
    "                    ReinforcementLearningBot(verbose=False, model_name='../players/PPO15v1', model_type='ppo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a29367-dacd-4c59-a1f3-8847f56d5bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name='../players/A2C15v2', model_type='a2c'), \n",
    "                    ReinforcementLearningBot(verbose=False, model_name='../players/DQN15v1', model_type='dqn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566ab4d7-d9db-4166-a3e1-1daf0e840568",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_bot_comparison(ReinforcementLearningBot(verbose=False, model_name='../players/A2C15v2', model_type='a2c'), HeuristicBot(verbose=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
